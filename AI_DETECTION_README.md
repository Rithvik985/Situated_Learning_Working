# AI Detection Testing - Summary

## âœ… What I've Created

I've created **4 comprehensive testing resources** for the AI detection system:

### 1. **Quick Start Guide** (`AI_DETECTION_QUICK_START.md`)
   - ğŸ“– Best place to start
   - ğŸ¯ 4 different testing methods (UI, Python, PowerShell, curl)
   - ğŸ” Understanding results & thresholds
   - âš ï¸ Troubleshooting tips

### 2. **Comprehensive Guide** (`AI_DETECTION_TEST_GUIDE.md`)
   - ğŸ“š Deep dive documentation
   - ğŸ—ï¸ Full architecture explanation
   - ğŸ§ª 4 testing methods with code examples
   - ğŸ› Troubleshooting for each issue
   - ğŸ“Š Risk assessment thresholds
   - ğŸ”— Resource links

### 3. **Python Test Script** (`test_ai_detection.py`)
   - ğŸ Full-featured test automation
   - âœ… Connection validation
   - ğŸ“Š Detailed result display
   - ğŸ’¾ Saves results to JSON file
   - ğŸ¨ Pretty-printed output

### 4. **Quick Test Script** (`quick_test_ai.py`)
   - âš¡ Minimal, fast test
   - ğŸ¨ Color-coded results
   - ğŸ“ Just 3 steps
   - ğŸ’¾ Auto-saves results

---

## ğŸš€ Fastest Way to Test (2 minutes)

### Prerequisites
1. Backend running: 
   ```powershell
   cd backend
   python start_evaluation_server.py
   ```

2. Have a student submission (create one via Student Workflow page)

### Then Run
```powershell
python quick_test_ai.py
```

**Done!** You'll see:
- âœ… Connection status
- âœ… AI probability (0-100%)
- âœ… Risk level (Low/Medium/High)
- âœ… Recommendations

---

## ğŸ¯ Three Testing Approaches

### Approach 1: UI Testing (Visual)
```
Frontend â†’ Faculty Evaluation page â†’ Select submission â†’ "Detect AI Content" button
```
**Best for**: Manual testing, seeing UI in action

### Approach 2: Automated Testing (Python)
```
python test_ai_detection.py
```
**Best for**: Batch testing, CI/CD integration, detailed logs

### Approach 3: API Testing (curl/PowerShell)
```powershell
curl http://localhost:8000/api/faculty/pending-submissions
curl -X POST http://localhost:8000/api/faculty/submissions/{id}/detect-ai
```
**Best for**: Manual API exploration, debugging

---

## ğŸ“Š How the System Works

```
Student Submission (text)
    â†“
RADAR-Vicuna-7B Model (HuggingFace)
    â†“
AI Probability (0.0 - 1.0)
    â†“
Risk Assessment:
  - Low (< 0.7)     â†’ Human written
  - Medium (0.7-0.9) â†’ Review carefully  
  - High (> 0.9)    â†’ Likely AI
    â†“
Saved to Database (student_submissions.ai_detection_results)
```

---

## ğŸ”‘ Key Endpoints

| Method | Endpoint | Purpose |
|---|---|---|
| GET | `/api/faculty/pending-submissions` | List submissions awaiting evaluation |
| POST | `/api/faculty/submissions/{id}/detect-ai` | Run AI detection analysis |

---

## ğŸ“‹ What You Can Test

1. âœ… **Human-written text** â†’ Low AI probability (< 0.7)
2. âœ… **AI-generated text** â†’ High AI probability (> 0.8)
3. âœ… **Mixed content** â†’ Medium probability (0.7-0.8)
4. âœ… **Different lengths** â†’ Works with any submission size
5. âœ… **Database persistence** â†’ Results saved to DB

---

## ğŸ› ï¸ Files to Check

| File | Purpose | Key Info |
|---|---|---|
| `backend/services/radar_service.py` | Core detection logic | Lines 14-50 for model config |
| `backend/routers/faculty.py` | API endpoint | Line ~629 for detect-ai endpoint |
| `frontend/src/pages/FacultyEvaluation.jsx` | UI button | `detectAIContent` function ~line 168 |
| `database/models.py` | DB schema | `ai_detection_results` column in StudentSubmission |

---

## âš¡ Common Issues & Fixes

| Issue | Fix |
|---|---|
| **"No pending submissions"** | Create one in Student Workflow first |
| **"Connection refused"** | Start backend: `python start_evaluation_server.py` |
| **"Timeout"** | Normal on first run (model loads) - wait 1-2 min |
| **"404 Error"** | Invalid submission ID |
| **Slow GPU memory** | Falls back to CPU auto |

---

## ğŸ“ˆ Next Steps

1. âœ… Run `python quick_test_ai.py` (2 min)
2. âœ… Try UI testing in Faculty Evaluation page
3. âœ… Run `python test_ai_detection.py` for automation
4. âœ… Check database: query `ai_detection_results` column
5. âœ… Customize thresholds in `radar_service.py` if needed

---

## ğŸ“ Learning Resources

- **RADAR Model**: https://huggingface.co/TrustSafeAI/RADAR-Vicuna-7B
- **Transformers Library**: https://huggingface.co/docs/transformers/
- **PyTorch**: https://pytorch.org/
- **FastAPI**: https://fastapi.tiangolo.com/

---

## ğŸ’¡ Pro Tips

1. **First run takes time** - Model downloads (~7GB) on first execution
2. **GPU optional** - Works fine on CPU if GPU not available
3. **Batch testing** - Modify test scripts to loop through multiple submissions
4. **Customize thresholds** - Edit threshold in `radar_service.py` line 43
5. **Monitor logs** - Check backend console for detailed debug info

---

## ğŸ¯ Summary

You now have **multiple ways** to test AI detection:

- ğŸ“– **Read**: `AI_DETECTION_QUICK_START.md` (best place to start)
- ğŸš€ **Quick Test**: `python quick_test_ai.py` (2 minutes)
- ğŸ”¬ **Full Test**: `python test_ai_detection.py` (automated)
- ğŸ–¥ï¸ **UI Test**: Faculty Evaluation page "Detect AI Content" button
- ğŸ“š **Learn**: `AI_DETECTION_TEST_GUIDE.md` (comprehensive guide)

**Recommendation**: Start with `quick_test_ai.py` to see it work, then explore other methods!
